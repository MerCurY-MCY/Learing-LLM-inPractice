{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Baby', ',', 'I', 'don', \"'\", 't', 'feel', 'so', 'goodsix', 'words', 'you', 'never', 'understoodI', \"'\", 'll', 'never', 'let', 'you', 'gofive', 'words', 'you', \"'\", 'll', 'never', 'say', '(', 'aww', ')', 'I', 'laugh', 'along', 'like', 'nothing', \"'\", 's', 'wrongfour', 'days', 'has', 'never', 'felt', 'so', 'longIf', 'three', \"'\", 's', 'a', 'crowd', 'and', 'two', 'was', 'usone', 'slipped', 'away', '(', 'hahahahaha', ')', 'I', 'just', 'wanna', 'make', 'you', 'feel', 'okayBut', 'all', 'you', 'do', 'is', 'look', 'the', 'other', 'wayI', 'can', \"'\", 't', 'tell', 'you', 'how', 'much', 'I', 'wish', 'I', 'didn', \"'\", 't', 'wanna', 'stayI', 'just', 'kinda', 'wish', 'you', 'were', 'gayIs', 'there', 'a', 'reason', 'we', \"'\", 're', 'not', 'through', '?', 'Is', 'there', 'a', '12', '-', 'step', 'just', 'for', 'you', '?', 'Our', 'conversation', \"'\", 's', 'all', 'in', 'blue11', '\"', 'heys', '\"', '(', 'Hey', ',', 'hey', ',', 'hey', ',', 'hey', ')', 'Ten', 'fingers', 'tearin', \"'\", 'out', 'my', 'hairNine', 'times', ',', 'you', 'never', 'made', 'it', 'thereI', 'ate', 'alone', 'at', 'seven', ',', 'you', 'were', 'six', 'minutes', 'awayHow', 'am', 'I', 'supposed', 'to', 'make', 'you', 'feel', 'okayWhen', 'all', 'you', 'do', 'is', 'walk', 'the', 'other', 'way', '?', 'I', 'can', \"'\", 't', 'tell', 'you', 'how', 'much', 'I', 'wish', 'I', 'didn', \"'\", 't', 'wanna', 'stayI', 'just', 'kinda', 'wish', 'you', 'were', 'gayTo', 'spare', 'my', 'prideTo', 'give', 'your', 'lack', 'of', 'interest', ',', 'an', 'explanationDon', \"'\", 't', 'say', 'I', \"'\", 'm', 'not', 'your', 'typeJust', 'say', 'that', 'I', \"'\", 'm', 'not', 'your', 'preferred', 'sexual', 'orientationI', \"'\", 'm', 'so', 'selfishBut', 'you', 'make', 'me', 'feel', 'helpless', ',', 'yeahAnd', 'I', 'can', \"'\", 't', 'stand', 'another', 'dayStand', 'another', 'dayI', 'just', 'wanna', 'make', 'you', 'feel', 'okayBut', 'all', 'you', 'do', 'is', 'look', 'the', 'other', 'way', ',', 'hmmI', 'can', \"'\", 't', 'tell', 'you', 'how', 'much', 'I', 'wish', 'I', 'didn', \"'\", 't', 'wanna', 'stayI', 'just', 'kinda', 'wish', 'you', 'were', 'gayI', 'just', 'kinda', 'wish', 'you', 'were', 'gayI', 'just', 'kinda', 'wish', 'you', 'were', 'gay']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import toolz #处理迭代器、字典、列表等数据结构的工具\n",
    "'''\n",
    "re这个模块的主要作用是让你能够在 Python 程序中使用正则表达式来处理字符串。\n",
    "匹配 (Matching)：检查一个字符串是否符合某个特定的模式（格式）。\n",
    "搜索 (Searching)：在一个字符串中查找符合特定模式的部分。\n",
    "替换 (Substitution)：查找字符串中符合模式的部分，并将其替换为其他内容。\n",
    "分割 (Splitting)：根据模式将字符串分割成多个部分。\n",
    "\n",
    "re 模块提供了很多函数来实现这些功能，比如：\n",
    "re.match(): 从字符串开头尝试匹配模式。\n",
    "re.search(): 在整个字符串中搜索第一个匹配模式的位置。\n",
    "re.findall(): 查找字符串中所有匹配模式的子串，并以列表形式返回。\n",
    "re.split(): 根据模式分割字符串。\n",
    "re.sub(): 查找并替换。\n",
    "re.compile(): 编译正则表达式。\n",
    "'''\n",
    "def wordpunct_tokenize(text):\n",
    "    #\\w匹配Unicode字符     ^\\w\\s匹配非Unicode字符和非空白字符（标点符号）\n",
    "\n",
    "    _pattern = r\"\\w+|[^\\w\\s]+\" \n",
    "    \n",
    "    #编译为对象   re.MULTILINE表示$和^可以匹配下一行开头和结尾，re.DOTALL表示点号也可以匹配换行符\n",
    "    _regexp = re.compile(_pattern, flags=re.UNICODE | re.MULTILINE | re.DOTALL) \n",
    "    return _regexp.findall(text)\n",
    "\n",
    "corpus = [\n",
    "        \"Baby, I don't feel so good\", \n",
    "        \"six words you never understood\",\n",
    "        \"I'll never let you go\", \n",
    "        \"five words you'll never say (aww)\",\n",
    "        \"I laugh along like nothing's wrong\" ,\n",
    "        \"four days has never felt so long\",\n",
    "        \"If three's a crowd and two was us\",\n",
    "        \"one slipped away (hahahahaha)\",\n",
    "        \"I just wanna make you feel okay\",\n",
    "        \"But all you do is look the other way\",\n",
    "        \"I can't tell you how much I wish I didn't wanna stay\",\n",
    "        \"I just kinda wish you were gay\",\n",
    "        \"Is there a reason we're not through?\",\n",
    "        \"Is there a 12-step just for you?\",\n",
    "        \"Our conversation's all in blue\",\n",
    "        \"11 \\\"heys\\\" (Hey, hey, hey, hey)\",\n",
    "        \"Ten fingers tearin' out my hair\",\n",
    "        \"Nine times, you never made it there\",\n",
    "        \"I ate alone at seven, you were six minutes away\",\n",
    "        \"How am I supposed to make you feel okay\",\n",
    "        \"When all you do is walk the other way?\",\n",
    "        \"I can't tell you how much I wish I didn't wanna stay\",\n",
    "        \"I just kinda wish you were gay\",\n",
    "        \"To spare my pride\",\n",
    "        \"To give your lack of interest, an explanation\",\n",
    "        \"Don't say I'm not your type\",\n",
    "        \"Just say that I'm not your preferred sexual orientation\",\n",
    "        \"I'm so selfish\",\n",
    "        \"But you make me feel helpless, yeah\",\n",
    "        \"And I can't stand another day\",\n",
    "        \"Stand another day\",\n",
    "        \"I just wanna make you feel okay\",\n",
    "        \"But all you do is look the other way, hmm\",\n",
    "        \"I can't tell you how much I wish I didn't wanna stay\",\n",
    "        \"I just kinda wish you were gay\",\n",
    "        \"I just kinda wish you were gay\",\n",
    "        \"I just kinda wish you were gay\",\n",
    "]\n",
    "\n",
    "print(wordpunct_tokenize(''.join(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<MASK>', 9999), ('<CLS>', 9999), ('<PAD>', 9999), ('<SEP>', 9999), ('<UNK>', 9999), ('a', 27), ('i/w', 25), ('/w', 23), ('you/w', 21), ('o', 20), ('i', 20), (\"'/w\", 19), ('e', 17), ('r', 16), ('t/w', 15), ('l', 12), ('e/w', 12), ('s/w', 12), ('n', 11), ('u', 11), ('p', 11), ('g', 10), ('d', 10), ('s', 10), (',/w', 9), ('just/w', 9), ('n/w', 9), ('w', 8), ('h', 8), ('wish/w', 8), ('f', 7), ('t', 7), ('were/w', 6), ('is/w', 5), ('gay/w', 5), ('feel/w', 5), ('wanna/w', 5), ('never/w', 5), ('kinda/w', 5), ('m/w', 5), ('v', 4), ('y', 4), ('m', 4), ('a/w', 4), ('h/w', 4), ('all/w', 4), ('can/w', 4), ('hey/w', 4), ('how/w', 4), ('make/w', 4), ('c', 3), ('1', 3), ('k', 3), ('b', 3), ('th', 3), ('er', 3), ('way/w', 3), ('ne/w', 3), ('other/w', 3), ('the/w', 3), ('there/w', 3), ('he', 3), ('(/w', 3), (')/w', 3), ('?/w', 3), ('say/w', 3), ('so/w', 3), ('do/w', 3), ('to/w', 3), ('te', 3), ('okay/w', 3), ('stay/w', 3), ('but/w', 3), ('not/w', 3), ('your/w', 3), ('tell/w', 3), ('much/w', 3), ('didn/w', 3), ('lo', 3), ('x', 2), ('ll/w', 2), ('and/w', 2), ('another/w', 2), ('\"/w', 2), ('my/w', 2), ('day/w', 2), ('away/w', 2), ('stand/w', 2), ('don/w', 2), ('six/w', 2), ('look/w', 2), ('words/w', 2), ('2', 1), ('-', 1), ('y/w', 1), ('l/w', 1), ('is', 1), ('o/w', 1), ('an/w', 1), ('an', 1), ('st', 1), ('fe', 1), ('fer', 1), ('ma', 1), ('noth', 1), ('te/w', 1), ('ste', 1), ('ter', 1), ('st/w', 1), ('wo/w', 1)]\n"
     ]
    }
   ],
   "source": [
    "class BPEtokenizer():\n",
    "    special = ['<UKN>', '<END>', '<PAD>','<MAD>']  #特殊字符填充\n",
    "    \n",
    "    def __init__(self,vocab_size = 10000, lowercase = True,basic_tokenizer = wordpunct_tokenize,\n",
    "                 unk='<UNK>', sep='<SEP>', pad='<PAD>', cls='<CLS>', mask='<MASK>', user_specials=None):\n",
    "        self.vocal_size = vocab_size\n",
    "        self.lowercase = lowercase\n",
    "        self.tokenizer = basic_tokenizer\n",
    "        self.special = [unk, sep, pad, cls, mask]\n",
    "        \n",
    "\n",
    "\n",
    "    def loadAndTransform(self, vocab_fn=None, vocab=None):\n",
    "        if vocab:\n",
    "            self.vocab = vocab\n",
    "        else:\n",
    "            self.vocab = [l.strip() for l in open(vocab_fn, 'r')]\n",
    "        vocab_len = len(self. vocab)\n",
    "        self.voToid = {x: y for x, y in enumerate(self.vocab)} #把字符转换为索引\n",
    "        self.idTovo = {y: x for x, y in self.voto2d.items()}  #把索引转换为字符\n",
    "         \n",
    "    def train(self, corpus = list, max_step = 10000, out_fn = 'vocabulary.txt'):\n",
    "\n",
    "        #########################################统计词频################################################\n",
    "        if self.lowercase:\n",
    "            corpus = [s.lower() for s in corpus]\n",
    "        \n",
    "        #map用于把一个函数依次对一个数据结构中的元素使用，并返回一个迭代器\n",
    "        corpus = list(map(self.tokenizer,corpus))\n",
    "        \n",
    "        \n",
    "        #展平该列表\n",
    "        corpus = toolz.concat(corpus)\n",
    "        \n",
    "        #把每个元素转换成元组并加入结尾符，计算每个单词出现的次数。Counter返回一个元素计数的字典。\n",
    "        split_corpus = Counter(tuple(word) + ('/w',) for word in corpus)\n",
    "\n",
    "        #split_corpus = Counter([tuple(word)+ ('<\\W>', ) for word in toolz.concat(map(self.tokenizer, corpus))])\n",
    "        \n",
    "\n",
    "        ##########################################逐步合并高频词为token并生成词表#################################\n",
    "        vocab = self._count_vocab(split_corpus)\n",
    "\n",
    "        for i in range(max_step):\n",
    "            split_corpus, vocab_cnt = self._countAndMerge(split_corpus)   #保留单词结构，统计一个单词内出现的二元字词的词频\n",
    "            vocab = self._count_vocab(split_corpus)   #把单词切碎，只统计字符级别的词频\n",
    "            if len(vocab) > self.vocal_size or vocab_cnt < 0 : break\n",
    "        \n",
    "        ##########插入特殊词######################\n",
    "        for s in self.special:\n",
    "            if s not in vocab:\n",
    "                vocab.insert(0,(s,9999))\n",
    "        \n",
    "        #####导出列表#####\n",
    "\n",
    "        with open(out_fn,'w') as f:\n",
    "            f.write('\\n'.join(token for token, _ in vocab))\n",
    "        \n",
    "        self.vocab = [token for token, _ in vocab]\n",
    "\n",
    "        return vocab \n",
    "\n",
    "\n",
    "      \n",
    "    \n",
    "    def _count_vocab(self, split_corpus): \n",
    "        _countWord = Counter([data for data in toolz.concat([w * x  for w, x in split_corpus.items()])])      #.items()顺序访问字典中的元素，enumerate访问元组或者列表\n",
    "        _sortWord = sorted(_countWord.items(), key = lambda x : x[1], reverse = True) #按第一维度降序排序字符\n",
    "        return _sortWord\n",
    "    \n",
    "    def _countAndMerge(self, split_corpus):\n",
    "        ngram = 2\n",
    "        bigramCounter = Counter()\n",
    "\n",
    "        for token, count in split_corpus.items():  #循环扫描每个单词和其出现频率\n",
    "            if count < 2 : continue          #跳过小于2的子词\n",
    "            for subwords in toolz.sliding_window(ngram, token):  #使用2的滑动窗口在单词上滚动\n",
    "                bigramCounter[subwords] += count  #将每个长度为2的子词的出现次数记录下来\n",
    "\n",
    "        if len(bigramCounter) > 0 :\n",
    "            max_bigram_key = max(bigramCounter, key=bigramCounter.get)    #找出最大频率的二元子词，max会循环读取可迭代对象，并对每个对象执行key对应的函数，比较函数计算出的值。（对于字典，读出的是键）\n",
    "        else: return split_corpus , -1\n",
    "        \n",
    "        max_bigram_cnt = bigramCounter.get(max_bigram_key)\n",
    "        \n",
    "        list_split_corpus_key = list(split_corpus.keys())\n",
    "        for tokens in list_split_corpus_key:\n",
    "        \n",
    "            temp_tokens = ' '.join(tokens)   #jion方法可以把数据结构中的参数合并成字符串，.前面的符号是合并时每个元素间插入的字符\n",
    "            \n",
    "            temp_tokens = temp_tokens.replace(' '.join(max_bigram_key), ''.join(max_bigram_key))   #把原始的token中的分离字符替换为合并在一起的二元高频字符\n",
    "\n",
    "            new_tokens = tuple(temp_tokens.split(' '))  #split方法通过括号里的字符把字符串分开，返回一个列表，再转换成元组得到例如(I lo v e) 的形式，其中lo是之前统计得到的高频二元字词\n",
    "\n",
    "            #temp_split_corpus = tuple(' '.join(tokens).replace(' '.join(list_split_corpus_key), ''.join(list_split_corpus_key)).split(' '))\n",
    "            if  new_tokens != tokens:\n",
    "                split_corpus[new_tokens] = split_corpus[tokens]\n",
    "                split_corpus.pop(tokens)\n",
    "        return split_corpus, max_bigram_cnt\n",
    "    \n",
    "    \n",
    "BPE = BPEtokenizer()\n",
    "sequence = BPE.train(corpus = corpus)\n",
    "print(sequence)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['baby', ',', 'i', 'don', \"'\", 't', 'feel', 'so', 'good'], ['six', 'words', 'you', 'never', 'understood'], ['i', \"'\", 'll', 'never', 'let', 'you', 'go'], ['five', 'words', 'you', \"'\", 'll', 'never', 'say', '(', 'aww', ')'], ['i', 'laugh', 'along', 'like', 'nothing', \"'\", 's', 'wrong'], ['four', 'days', 'has', 'never', 'felt', 'so', 'long'], ['if', 'three', \"'\", 's', 'a', 'crowd', 'and', 'two', 'was', 'us'], ['one', 'slipped', 'away', '(', 'hahahahaha', ')'], ['i', 'just', 'wanna', 'make', 'you', 'feel', 'okay'], ['but', 'all', 'you', 'do', 'is', 'look', 'the', 'other', 'way'], ['i', 'can', \"'\", 't', 'tell', 'you', 'how', 'much', 'i', 'wish', 'i', 'didn', \"'\", 't', 'wanna', 'stay'], ['i', 'just', 'kinda', 'wish', 'you', 'were', 'gay'], ['is', 'there', 'a', 'reason', 'we', \"'\", 're', 'not', 'through', '?'], ['is', 'there', 'a', '12', '-', 'step', 'just', 'for', 'you', '?'], ['our', 'conversation', \"'\", 's', 'all', 'in', 'blue'], ['11', '\"', 'heys', '\"', '(', 'hey', ',', 'hey', ',', 'hey', ',', 'hey', ')'], ['ten', 'fingers', 'tearin', \"'\", 'out', 'my', 'hair'], ['nine', 'times', ',', 'you', 'never', 'made', 'it', 'there'], ['i', 'ate', 'alone', 'at', 'seven', ',', 'you', 'were', 'six', 'minutes', 'away'], ['how', 'am', 'i', 'supposed', 'to', 'make', 'you', 'feel', 'okay'], ['when', 'all', 'you', 'do', 'is', 'walk', 'the', 'other', 'way', '?'], ['i', 'can', \"'\", 't', 'tell', 'you', 'how', 'much', 'i', 'wish', 'i', 'didn', \"'\", 't', 'wanna', 'stay'], ['i', 'just', 'kinda', 'wish', 'you', 'were', 'gay'], ['to', 'spare', 'my', 'pride'], ['to', 'give', 'your', 'lack', 'of', 'interest', ',', 'an', 'explanation'], ['don', \"'\", 't', 'say', 'i', \"'\", 'm', 'not', 'your', 'type'], ['just', 'say', 'that', 'i', \"'\", 'm', 'not', 'your', 'preferred', 'sexual', 'orientation'], ['i', \"'\", 'm', 'so', 'selfish'], ['but', 'you', 'make', 'me', 'feel', 'helpless', ',', 'yeah'], ['and', 'i', 'can', \"'\", 't', 'stand', 'another', 'day'], ['stand', 'another', 'day'], ['i', 'just', 'wanna', 'make', 'you', 'feel', 'okay'], ['but', 'all', 'you', 'do', 'is', 'look', 'the', 'other', 'way', ',', 'hmm'], ['i', 'can', \"'\", 't', 'tell', 'you', 'how', 'much', 'i', 'wish', 'i', 'didn', \"'\", 't', 'wanna', 'stay'], ['i', 'just', 'kinda', 'wish', 'you', 'were', 'gay'], ['i', 'just', 'kinda', 'wish', 'you', 'were', 'gay'], ['i', 'just', 'kinda', 'wish', 'you', 'were', 'gay']]\n",
      "Counter({('i', '/w'): 25, ('y', 'o', 'u', '/w'): 21, (\"'\", '/w'): 19, (',', '/w'): 9, ('t', '/w'): 9, ('j', 'u', 's', 't', '/w'): 9, ('w', 'i', 's', 'h', '/w'): 8, ('w', 'e', 'r', 'e', '/w'): 6, ('f', 'e', 'e', 'l', '/w'): 5, ('n', 'e', 'v', 'e', 'r', '/w'): 5, ('w', 'a', 'n', 'n', 'a', '/w'): 5, ('i', 's', '/w'): 5, ('k', 'i', 'n', 'd', 'a', '/w'): 5, ('g', 'a', 'y', '/w'): 5, ('m', 'a', 'k', 'e', '/w'): 4, ('a', 'l', 'l', '/w'): 4, ('c', 'a', 'n', '/w'): 4, ('h', 'o', 'w', '/w'): 4, ('h', 'e', 'y', '/w'): 4, ('s', 'o', '/w'): 3, ('s', 'a', 'y', '/w'): 3, ('(', '/w'): 3, (')', '/w'): 3, ('s', '/w'): 3, ('a', '/w'): 3, ('o', 'k', 'a', 'y', '/w'): 3, ('b', 'u', 't', '/w'): 3, ('d', 'o', '/w'): 3, ('t', 'h', 'e', '/w'): 3, ('o', 't', 'h', 'e', 'r', '/w'): 3, ('w', 'a', 'y', '/w'): 3, ('t', 'e', 'l', 'l', '/w'): 3, ('m', 'u', 'c', 'h', '/w'): 3, ('d', 'i', 'd', 'n', '/w'): 3, ('s', 't', 'a', 'y', '/w'): 3, ('t', 'h', 'e', 'r', 'e', '/w'): 3, ('n', 'o', 't', '/w'): 3, ('?', '/w'): 3, ('t', 'o', '/w'): 3, ('y', 'o', 'u', 'r', '/w'): 3, ('m', '/w'): 3, ('d', 'o', 'n', '/w'): 2, ('s', 'i', 'x', '/w'): 2, ('w', 'o', 'r', 'd', 's', '/w'): 2, ('l', 'l', '/w'): 2, ('a', 'n', 'd', '/w'): 2, ('a', 'w', 'a', 'y', '/w'): 2, ('l', 'o', 'o', 'k', '/w'): 2, ('\"', '/w'): 2, ('m', 'y', '/w'): 2, ('s', 't', 'a', 'n', 'd', '/w'): 2, ('a', 'n', 'o', 't', 'h', 'e', 'r', '/w'): 2, ('d', 'a', 'y', '/w'): 2, ('b', 'a', 'b', 'y', '/w'): 1, ('g', 'o', 'o', 'd', '/w'): 1, ('u', 'n', 'd', 'e', 'r', 's', 't', 'o', 'o', 'd', '/w'): 1, ('l', 'e', 't', '/w'): 1, ('g', 'o', '/w'): 1, ('f', 'i', 'v', 'e', '/w'): 1, ('a', 'w', 'w', '/w'): 1, ('l', 'a', 'u', 'g', 'h', '/w'): 1, ('a', 'l', 'o', 'n', 'g', '/w'): 1, ('l', 'i', 'k', 'e', '/w'): 1, ('n', 'o', 't', 'h', 'i', 'n', 'g', '/w'): 1, ('w', 'r', 'o', 'n', 'g', '/w'): 1, ('f', 'o', 'u', 'r', '/w'): 1, ('d', 'a', 'y', 's', '/w'): 1, ('h', 'a', 's', '/w'): 1, ('f', 'e', 'l', 't', '/w'): 1, ('l', 'o', 'n', 'g', '/w'): 1, ('i', 'f', '/w'): 1, ('t', 'h', 'r', 'e', 'e', '/w'): 1, ('c', 'r', 'o', 'w', 'd', '/w'): 1, ('t', 'w', 'o', '/w'): 1, ('w', 'a', 's', '/w'): 1, ('u', 's', '/w'): 1, ('o', 'n', 'e', '/w'): 1, ('s', 'l', 'i', 'p', 'p', 'e', 'd', '/w'): 1, ('h', 'a', 'h', 'a', 'h', 'a', 'h', 'a', 'h', 'a', '/w'): 1, ('r', 'e', 'a', 's', 'o', 'n', '/w'): 1, ('w', 'e', '/w'): 1, ('r', 'e', '/w'): 1, ('t', 'h', 'r', 'o', 'u', 'g', 'h', '/w'): 1, ('1', '2', '/w'): 1, ('-', '/w'): 1, ('s', 't', 'e', 'p', '/w'): 1, ('f', 'o', 'r', '/w'): 1, ('o', 'u', 'r', '/w'): 1, ('c', 'o', 'n', 'v', 'e', 'r', 's', 'a', 't', 'i', 'o', 'n', '/w'): 1, ('i', 'n', '/w'): 1, ('b', 'l', 'u', 'e', '/w'): 1, ('1', '1', '/w'): 1, ('h', 'e', 'y', 's', '/w'): 1, ('t', 'e', 'n', '/w'): 1, ('f', 'i', 'n', 'g', 'e', 'r', 's', '/w'): 1, ('t', 'e', 'a', 'r', 'i', 'n', '/w'): 1, ('o', 'u', 't', '/w'): 1, ('h', 'a', 'i', 'r', '/w'): 1, ('n', 'i', 'n', 'e', '/w'): 1, ('t', 'i', 'm', 'e', 's', '/w'): 1, ('m', 'a', 'd', 'e', '/w'): 1, ('i', 't', '/w'): 1, ('a', 't', 'e', '/w'): 1, ('a', 'l', 'o', 'n', 'e', '/w'): 1, ('a', 't', '/w'): 1, ('s', 'e', 'v', 'e', 'n', '/w'): 1, ('m', 'i', 'n', 'u', 't', 'e', 's', '/w'): 1, ('a', 'm', '/w'): 1, ('s', 'u', 'p', 'p', 'o', 's', 'e', 'd', '/w'): 1, ('w', 'h', 'e', 'n', '/w'): 1, ('w', 'a', 'l', 'k', '/w'): 1, ('s', 'p', 'a', 'r', 'e', '/w'): 1, ('p', 'r', 'i', 'd', 'e', '/w'): 1, ('g', 'i', 'v', 'e', '/w'): 1, ('l', 'a', 'c', 'k', '/w'): 1, ('o', 'f', '/w'): 1, ('i', 'n', 't', 'e', 'r', 'e', 's', 't', '/w'): 1, ('a', 'n', '/w'): 1, ('e', 'x', 'p', 'l', 'a', 'n', 'a', 't', 'i', 'o', 'n', '/w'): 1, ('t', 'y', 'p', 'e', '/w'): 1, ('t', 'h', 'a', 't', '/w'): 1, ('p', 'r', 'e', 'f', 'e', 'r', 'r', 'e', 'd', '/w'): 1, ('s', 'e', 'x', 'u', 'a', 'l', '/w'): 1, ('o', 'r', 'i', 'e', 'n', 't', 'a', 't', 'i', 'o', 'n', '/w'): 1, ('s', 'e', 'l', 'f', 'i', 's', 'h', '/w'): 1, ('m', 'e', '/w'): 1, ('h', 'e', 'l', 'p', 'l', 'e', 's', 's', '/w'): 1, ('y', 'e', 'a', 'h', '/w'): 1, ('h', 'm', 'm', '/w'): 1})\n"
     ]
    }
   ],
   "source": [
    "def countTokens(corpus):\n",
    "    \n",
    "        corpus = [s.lower() for s in corpus]\n",
    "        \n",
    "        #map用于把一个函数依次对一个数据结构中的元素使用，并返回一个迭代器\n",
    "        corpus = list(map(wordpunct_tokenize,corpus))\n",
    "        print(corpus)\n",
    "        #展平该列表\n",
    "        corpus = toolz.concat(corpus)\n",
    "        \n",
    "        #把每个元素转换成元组并加入结尾符，计算每个单词出现的次数\n",
    "        split_corpus = Counter(tuple(word) + ('/w',) for word in corpus)\n",
    "        return split_corpus\n",
    "print (countTokens(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'/w': 238, 'e': 103, 'a': 90, 'o': 82, 'n': 64, 's': 63, 'u': 50, 'h': 49, 'r': 45, 'i': 44, 'l': 41, 't': 40, 'w': 39, 'd': 34, 't/w': 31, 'y': 28, 'y/w': 28, 'i/w': 25, \"'\": 19, 'm': 19, 'k': 17, 'g': 15, 'f': 14, 'p': 11, 'c': 10, ',': 9, 'v': 9, 'j': 9, 'b': 6, 'x': 4, '(': 3, ')': 3, '?': 3, '1': 3, '\"': 2, '2': 1, '-': 1})\n",
      "[(',', '/w', ',', '/w', ',', '/w', ',', '/w', ',', '/w', ',', '/w', ',', '/w', ',', '/w', ',', '/w'), ('d', 'o', 'n', '/w', 'd', 'o', 'n', '/w'), (\"'\", '/w', \"'\", '/w', \"'\", '/w', \"'\", '/w', \"'\", '/w', \"'\", '/w', \"'\", '/w', \"'\", '/w', \"'\", '/w', \"'\", '/w', \"'\", '/w', \"'\", '/w', \"'\", '/w', \"'\", '/w', \"'\", '/w', \"'\", '/w', \"'\", '/w', \"'\", '/w', \"'\", '/w'), ('f', 'e', 'e', 'l', '/w', 'f', 'e', 'e', 'l', '/w', 'f', 'e', 'e', 'l', '/w', 'f', 'e', 'e', 'l', '/w', 'f', 'e', 'e', 'l', '/w'), ('s', 'o', '/w', 's', 'o', '/w', 's', 'o', '/w'), ('g', 'o', 'o', 'd', '/w'), ('s', 'i', 'x', '/w', 's', 'i', 'x', '/w'), ('w', 'o', 'r', 'd', 's', '/w', 'w', 'o', 'r', 'd', 's', '/w'), ('y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w', 'y', 'o', 'u', '/w'), ('n', 'e', 'v', 'e', 'r', '/w', 'n', 'e', 'v', 'e', 'r', '/w', 'n', 'e', 'v', 'e', 'r', '/w', 'n', 'e', 'v', 'e', 'r', '/w', 'n', 'e', 'v', 'e', 'r', '/w'), ('u', 'n', 'd', 'e', 'r', 's', 't', 'o', 'o', 'd', '/w'), ('l', 'l', '/w', 'l', 'l', '/w'), ('g', 'o', '/w'), ('f', 'i', 'v', 'e', '/w'), ('(', '/w', '(', '/w', '(', '/w'), ('a', 'w', 'w', '/w'), (')', '/w', ')', '/w', ')', '/w'), ('l', 'a', 'u', 'g', 'h', '/w'), ('a', 'l', 'o', 'n', 'g', '/w'), ('l', 'i', 'k', 'e', '/w'), ('n', 'o', 't', 'h', 'i', 'n', 'g', '/w'), ('s', '/w', 's', '/w', 's', '/w'), ('w', 'r', 'o', 'n', 'g', '/w'), ('f', 'o', 'u', 'r', '/w'), ('d', 'a', 'y', 's', '/w'), ('h', 'a', 's', '/w'), ('l', 'o', 'n', 'g', '/w'), ('i', 'f', '/w'), ('t', 'h', 'r', 'e', 'e', '/w'), ('a', '/w', 'a', '/w', 'a', '/w'), ('c', 'r', 'o', 'w', 'd', '/w'), ('a', 'n', 'd', '/w', 'a', 'n', 'd', '/w'), ('t', 'w', 'o', '/w'), ('w', 'a', 's', '/w'), ('u', 's', '/w'), ('o', 'n', 'e', '/w'), ('s', 'l', 'i', 'p', 'p', 'e', 'd', '/w'), ('h', 'a', 'h', 'a', 'h', 'a', 'h', 'a', 'h', 'a', '/w'), ('w', 'a', 'n', 'n', 'a', '/w', 'w', 'a', 'n', 'n', 'a', '/w', 'w', 'a', 'n', 'n', 'a', '/w', 'w', 'a', 'n', 'n', 'a', '/w', 'w', 'a', 'n', 'n', 'a', '/w'), ('m', 'a', 'k', 'e', '/w', 'm', 'a', 'k', 'e', '/w', 'm', 'a', 'k', 'e', '/w', 'm', 'a', 'k', 'e', '/w'), ('a', 'l', 'l', '/w', 'a', 'l', 'l', '/w', 'a', 'l', 'l', '/w', 'a', 'l', 'l', '/w'), ('d', 'o', '/w', 'd', 'o', '/w', 'd', 'o', '/w'), ('i', 's', '/w', 'i', 's', '/w', 'i', 's', '/w', 'i', 's', '/w', 'i', 's', '/w'), ('l', 'o', 'o', 'k', '/w', 'l', 'o', 'o', 'k', '/w'), ('t', 'h', 'e', '/w', 't', 'h', 'e', '/w', 't', 'h', 'e', '/w'), ('o', 't', 'h', 'e', 'r', '/w', 'o', 't', 'h', 'e', 'r', '/w', 'o', 't', 'h', 'e', 'r', '/w'), ('c', 'a', 'n', '/w', 'c', 'a', 'n', '/w', 'c', 'a', 'n', '/w', 'c', 'a', 'n', '/w'), ('t', 'e', 'l', 'l', '/w', 't', 'e', 'l', 'l', '/w', 't', 'e', 'l', 'l', '/w'), ('h', 'o', 'w', '/w', 'h', 'o', 'w', '/w', 'h', 'o', 'w', '/w', 'h', 'o', 'w', '/w'), ('m', 'u', 'c', 'h', '/w', 'm', 'u', 'c', 'h', '/w', 'm', 'u', 'c', 'h', '/w'), ('w', 'i', 's', 'h', '/w', 'w', 'i', 's', 'h', '/w', 'w', 'i', 's', 'h', '/w', 'w', 'i', 's', 'h', '/w', 'w', 'i', 's', 'h', '/w', 'w', 'i', 's', 'h', '/w', 'w', 'i', 's', 'h', '/w', 'w', 'i', 's', 'h', '/w'), ('d', 'i', 'd', 'n', '/w', 'd', 'i', 'd', 'n', '/w', 'd', 'i', 'd', 'n', '/w'), ('k', 'i', 'n', 'd', 'a', '/w', 'k', 'i', 'n', 'd', 'a', '/w', 'k', 'i', 'n', 'd', 'a', '/w', 'k', 'i', 'n', 'd', 'a', '/w', 'k', 'i', 'n', 'd', 'a', '/w'), ('w', 'e', 'r', 'e', '/w', 'w', 'e', 'r', 'e', '/w', 'w', 'e', 'r', 'e', '/w', 'w', 'e', 'r', 'e', '/w', 'w', 'e', 'r', 'e', '/w', 'w', 'e', 'r', 'e', '/w'), ('t', 'h', 'e', 'r', 'e', '/w', 't', 'h', 'e', 'r', 'e', '/w', 't', 'h', 'e', 'r', 'e', '/w'), ('r', 'e', 'a', 's', 'o', 'n', '/w'), ('w', 'e', '/w'), ('r', 'e', '/w'), ('t', 'h', 'r', 'o', 'u', 'g', 'h', '/w'), ('?', '/w', '?', '/w', '?', '/w'), ('1', '2', '/w'), ('-', '/w'), ('s', 't', 'e', 'p', '/w'), ('f', 'o', 'r', '/w'), ('o', 'u', 'r', '/w'), ('c', 'o', 'n', 'v', 'e', 'r', 's', 'a', 't', 'i', 'o', 'n', '/w'), ('i', 'n', '/w'), ('b', 'l', 'u', 'e', '/w'), ('1', '1', '/w'), ('\"', '/w', '\"', '/w'), ('h', 'e', 'y', 's', '/w'), ('t', 'e', 'n', '/w'), ('f', 'i', 'n', 'g', 'e', 'r', 's', '/w'), ('t', 'e', 'a', 'r', 'i', 'n', '/w'), ('h', 'a', 'i', 'r', '/w'), ('n', 'i', 'n', 'e', '/w'), ('t', 'i', 'm', 'e', 's', '/w'), ('m', 'a', 'd', 'e', '/w'), ('a', 't', 'e', '/w'), ('a', 'l', 'o', 'n', 'e', '/w'), ('s', 'e', 'v', 'e', 'n', '/w'), ('m', 'i', 'n', 'u', 't', 'e', 's', '/w'), ('a', 'm', '/w'), ('s', 'u', 'p', 'p', 'o', 's', 'e', 'd', '/w'), ('t', 'o', '/w', 't', 'o', '/w', 't', 'o', '/w'), ('w', 'h', 'e', 'n', '/w'), ('w', 'a', 'l', 'k', '/w'), ('s', 'p', 'a', 'r', 'e', '/w'), ('p', 'r', 'i', 'd', 'e', '/w'), ('g', 'i', 'v', 'e', '/w'), ('y', 'o', 'u', 'r', '/w', 'y', 'o', 'u', 'r', '/w', 'y', 'o', 'u', 'r', '/w'), ('l', 'a', 'c', 'k', '/w'), ('o', 'f', '/w'), ('a', 'n', '/w'), ('e', 'x', 'p', 'l', 'a', 'n', 'a', 't', 'i', 'o', 'n', '/w'), ('m', '/w', 'm', '/w', 'm', '/w'), ('t', 'y', 'p', 'e', '/w'), ('p', 'r', 'e', 'f', 'e', 'r', 'r', 'e', 'd', '/w'), ('s', 'e', 'x', 'u', 'a', 'l', '/w'), ('o', 'r', 'i', 'e', 'n', 't', 'a', 't', 'i', 'o', 'n', '/w'), ('s', 'e', 'l', 'f', 'i', 's', 'h', '/w'), ('m', 'e', '/w'), ('h', 'e', 'l', 'p', 'l', 'e', 's', 's', '/w'), ('y', 'e', 'a', 'h', '/w'), ('s', 't', 'a', 'n', 'd', '/w', 's', 't', 'a', 'n', 'd', '/w'), ('a', 'n', 'o', 't', 'h', 'e', 'r', '/w', 'a', 'n', 'o', 't', 'h', 'e', 'r', '/w'), ('h', 'm', 'm', '/w'), ('b', 'a', 'b', 'y/w'), ('s', 'a', 'y/w', 's', 'a', 'y/w', 's', 'a', 'y/w'), ('a', 'w', 'a', 'y/w', 'a', 'w', 'a', 'y/w'), ('o', 'k', 'a', 'y/w', 'o', 'k', 'a', 'y/w', 'o', 'k', 'a', 'y/w'), ('w', 'a', 'y/w', 'w', 'a', 'y/w', 'w', 'a', 'y/w'), ('s', 't', 'a', 'y/w', 's', 't', 'a', 'y/w', 's', 't', 'a', 'y/w'), ('g', 'a', 'y/w', 'g', 'a', 'y/w', 'g', 'a', 'y/w', 'g', 'a', 'y/w', 'g', 'a', 'y/w'), ('h', 'e', 'y/w', 'h', 'e', 'y/w', 'h', 'e', 'y/w', 'h', 'e', 'y/w'), ('m', 'y/w', 'm', 'y/w'), ('d', 'a', 'y/w', 'd', 'a', 'y/w'), ('i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w', 'i/w'), ('t/w', 't/w', 't/w', 't/w', 't/w', 't/w', 't/w', 't/w', 't/w'), ('l', 'e', 't/w'), ('f', 'e', 'l', 't/w'), ('j', 'u', 's', 't/w', 'j', 'u', 's', 't/w', 'j', 'u', 's', 't/w', 'j', 'u', 's', 't/w', 'j', 'u', 's', 't/w', 'j', 'u', 's', 't/w', 'j', 'u', 's', 't/w', 'j', 'u', 's', 't/w', 'j', 'u', 's', 't/w'), ('b', 'u', 't/w', 'b', 'u', 't/w', 'b', 'u', 't/w'), ('n', 'o', 't/w', 'n', 'o', 't/w', 'n', 'o', 't/w'), ('o', 'u', 't/w'), ('i', 't/w'), ('a', 't/w'), ('i', 'n', 't', 'e', 'r', 'e', 's', 't/w'), ('t', 'h', 'a', 't/w')]\n",
      "[('/w', 238), ('e', 103), ('a', 90), ('o', 82), ('n', 64), ('s', 63), ('u', 50), ('h', 49), ('r', 45), ('i', 44), ('l', 41), ('t', 40), ('w', 39), ('d', 34), ('t/w', 31), ('y', 28), ('y/w', 28), ('i/w', 25), (\"'\", 19), ('m', 19), ('k', 17), ('g', 15), ('f', 14), ('p', 11), ('c', 10), (',', 9), ('v', 9), ('j', 9), ('b', 6), ('x', 4), ('(', 3), (')', 3), ('?', 3), ('1', 3), ('\"', 2), ('2', 1), ('-', 1)]\n"
     ]
    }
   ],
   "source": [
    "def _count_vocab(split_corpus): \n",
    "    _countWord = Counter([data for data in toolz.concat([w * x  for w, x in split_corpus.items()])])      #.items()顺序访问字典中的元素，enumerate访问元组或者列表\n",
    "    print(_countWord)\n",
    "    print(([w * x  for w, x in split_corpus.items()]))\n",
    "    _sortWord = sorted(_countWord.items(), key = lambda x : x[1], reverse = True) #按第一维度降序排序字符，返回列表\n",
    "    return _sortWord\n",
    "\n",
    "print(_count_vocab(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Counter({('i/w',): 25, ('yo', 'u', '/w'): 21, (\"'\", '/w'): 19, (',', '/w'): 9, ('t/w',): 9, ('j', 'u', 's', 't/w'): 9, ('w', 'i', 's', 'h', '/w'): 8, ('w', 'e', 'r', 'e', '/w'): 6, ('f', 'e', 'e', 'l', '/w'): 5, ('n', 'e', 'v', 'e', 'r', '/w'): 5, ('w', 'a', 'n', 'n', 'a', '/w'): 5, ('i', 's', '/w'): 5, ('k', 'i', 'n', 'd', 'a', '/w'): 5, ('g', 'a', 'y/w'): 5, ('m', 'a', 'k', 'e', '/w'): 4, ('a', 'l', 'l', '/w'): 4, ('c', 'a', 'n', '/w'): 4, ('h', 'o', 'w', '/w'): 4, ('h', 'e', 'y/w'): 4, ('s', 'o', '/w'): 3, ('(', '/w'): 3, (')', '/w'): 3, ('s', '/w'): 3, ('a', '/w'): 3, ('d', 'o', '/w'): 3, ('t', 'h', 'e', '/w'): 3, ('o', 't', 'h', 'e', 'r', '/w'): 3, ('t', 'e', 'l', 'l', '/w'): 3, ('m', 'u', 'c', 'h', '/w'): 3, ('d', 'i', 'd', 'n', '/w'): 3, ('t', 'h', 'e', 'r', 'e', '/w'): 3, ('?', '/w'): 3, ('t', 'o', '/w'): 3, ('m', '/w'): 3, ('s', 'a', 'y/w'): 3, ('o', 'k', 'a', 'y/w'): 3, ('w', 'a', 'y/w'): 3, ('s', 't', 'a', 'y/w'): 3, ('b', 'u', 't/w'): 3, ('n', 'o', 't/w'): 3, ('yo', 'u', 'r', '/w'): 3, ('d', 'o', 'n', '/w'): 2, ('s', 'i', 'x', '/w'): 2, ('w', 'o', 'r', 'd', 's', '/w'): 2, ('l', 'l', '/w'): 2, ('a', 'n', 'd', '/w'): 2, ('l', 'o', 'o', 'k', '/w'): 2, ('\"', '/w'): 2, ('s', 't', 'a', 'n', 'd', '/w'): 2, ('a', 'n', 'o', 't', 'h', 'e', 'r', '/w'): 2, ('a', 'w', 'a', 'y/w'): 2, ('m', 'y/w'): 2, ('d', 'a', 'y/w'): 2, ('g', 'o', 'o', 'd', '/w'): 1, ('u', 'n', 'd', 'e', 'r', 's', 't', 'o', 'o', 'd', '/w'): 1, ('g', 'o', '/w'): 1, ('f', 'i', 'v', 'e', '/w'): 1, ('a', 'w', 'w', '/w'): 1, ('l', 'a', 'u', 'g', 'h', '/w'): 1, ('a', 'l', 'o', 'n', 'g', '/w'): 1, ('l', 'i', 'k', 'e', '/w'): 1, ('n', 'o', 't', 'h', 'i', 'n', 'g', '/w'): 1, ('w', 'r', 'o', 'n', 'g', '/w'): 1, ('f', 'o', 'u', 'r', '/w'): 1, ('d', 'a', 'y', 's', '/w'): 1, ('h', 'a', 's', '/w'): 1, ('l', 'o', 'n', 'g', '/w'): 1, ('i', 'f', '/w'): 1, ('t', 'h', 'r', 'e', 'e', '/w'): 1, ('c', 'r', 'o', 'w', 'd', '/w'): 1, ('t', 'w', 'o', '/w'): 1, ('w', 'a', 's', '/w'): 1, ('u', 's', '/w'): 1, ('o', 'n', 'e', '/w'): 1, ('s', 'l', 'i', 'p', 'p', 'e', 'd', '/w'): 1, ('h', 'a', 'h', 'a', 'h', 'a', 'h', 'a', 'h', 'a', '/w'): 1, ('r', 'e', 'a', 's', 'o', 'n', '/w'): 1, ('w', 'e', '/w'): 1, ('r', 'e', '/w'): 1, ('t', 'h', 'r', 'o', 'u', 'g', 'h', '/w'): 1, ('1', '2', '/w'): 1, ('-', '/w'): 1, ('s', 't', 'e', 'p', '/w'): 1, ('f', 'o', 'r', '/w'): 1, ('o', 'u', 'r', '/w'): 1, ('c', 'o', 'n', 'v', 'e', 'r', 's', 'a', 't', 'i', 'o', 'n', '/w'): 1, ('i', 'n', '/w'): 1, ('b', 'l', 'u', 'e', '/w'): 1, ('1', '1', '/w'): 1, ('h', 'e', 'y', 's', '/w'): 1, ('t', 'e', 'n', '/w'): 1, ('f', 'i', 'n', 'g', 'e', 'r', 's', '/w'): 1, ('t', 'e', 'a', 'r', 'i', 'n', '/w'): 1, ('h', 'a', 'i', 'r', '/w'): 1, ('n', 'i', 'n', 'e', '/w'): 1, ('t', 'i', 'm', 'e', 's', '/w'): 1, ('m', 'a', 'd', 'e', '/w'): 1, ('a', 't', 'e', '/w'): 1, ('a', 'l', 'o', 'n', 'e', '/w'): 1, ('s', 'e', 'v', 'e', 'n', '/w'): 1, ('m', 'i', 'n', 'u', 't', 'e', 's', '/w'): 1, ('a', 'm', '/w'): 1, ('s', 'u', 'p', 'p', 'o', 's', 'e', 'd', '/w'): 1, ('w', 'h', 'e', 'n', '/w'): 1, ('w', 'a', 'l', 'k', '/w'): 1, ('s', 'p', 'a', 'r', 'e', '/w'): 1, ('p', 'r', 'i', 'd', 'e', '/w'): 1, ('g', 'i', 'v', 'e', '/w'): 1, ('l', 'a', 'c', 'k', '/w'): 1, ('o', 'f', '/w'): 1, ('a', 'n', '/w'): 1, ('e', 'x', 'p', 'l', 'a', 'n', 'a', 't', 'i', 'o', 'n', '/w'): 1, ('t', 'y', 'p', 'e', '/w'): 1, ('p', 'r', 'e', 'f', 'e', 'r', 'r', 'e', 'd', '/w'): 1, ('s', 'e', 'x', 'u', 'a', 'l', '/w'): 1, ('o', 'r', 'i', 'e', 'n', 't', 'a', 't', 'i', 'o', 'n', '/w'): 1, ('s', 'e', 'l', 'f', 'i', 's', 'h', '/w'): 1, ('m', 'e', '/w'): 1, ('h', 'e', 'l', 'p', 'l', 'e', 's', 's', '/w'): 1, ('y', 'e', 'a', 'h', '/w'): 1, ('h', 'm', 'm', '/w'): 1, ('b', 'a', 'b', 'y/w'): 1, ('l', 'e', 't/w'): 1, ('f', 'e', 'l', 't/w'): 1, ('o', 'u', 't/w'): 1, ('i', 't/w'): 1, ('a', 't/w'): 1, ('i', 'n', 't', 'e', 'r', 'e', 's', 't/w'): 1, ('t', 'h', 'a', 't/w'): 1}), 24)\n"
     ]
    }
   ],
   "source": [
    "def _countAndMerge( split_corpus):\n",
    "    ngram = 2\n",
    "    bigramCounter = Counter()\n",
    "\n",
    "    for token, count in split_corpus.items():  #循环扫描每个单词和其出现频率\n",
    "        if count < 2 : continue          #跳过小于2的子词\n",
    "        for subwords in toolz.sliding_window(ngram, token):  #使用2的滑动窗口在单词上滚动\n",
    "            bigramCounter[subwords] += count  #将每个长度为2的子词的出现次数记录下来\n",
    "\n",
    "    if len(bigramCounter) > 0 :\n",
    "        max_bigram_key = max(bigramCounter, key=bigramCounter.get)    #找出最大频率的二元子词，max会循环读取可迭代对象，并对每个对象执行key对应的函数，比较函数计算出的值。（对于字典，读出的是键）\n",
    "    else: return split_corpus , -1\n",
    "    \n",
    "    max_bigram_cnt = bigramCounter.get(max_bigram_key)\n",
    "    \n",
    "    list_split_corpus_key = list(split_corpus.keys())\n",
    "    for tokens in list_split_corpus_key:\n",
    "       \n",
    "        temp_tokens = ' '.join(tokens)   #jion方法可以把数据结构中的参数合并成字符串，.前面的符号是合并时每个元素间插入的字符\n",
    "        \n",
    "        temp_tokens = temp_tokens.replace(' '.join(max_bigram_key), ''.join(max_bigram_key))   #把原始的token中的分离字符替换为合并在一起的二元高频字符\n",
    "\n",
    "        new_tokens = tuple(temp_tokens.split(' '))  #split方法通过括号里的字符把字符串分开，返回一个列表，再转换成元组得到例如(I lo v e) 的形式，其中lo是之前统计得到的高频二元字词\n",
    "\n",
    "        #temp_split_corpus = tuple(' '.join(tokens).replace(' '.join(list_split_corpus_key), ''.join(list_split_corpus_key)).split(' '))\n",
    "        if  new_tokens != tokens:\n",
    "            split_corpus[new_tokens] = split_corpus[tokens]\n",
    "            split_corpus.pop(tokens)\n",
    "    return split_corpus, max_bigram_cnt\n",
    "\n",
    "print(_countAndMerge(sequence))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
