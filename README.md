# Reproduce-classicLLM

## Repository Introduction

This repository is dedicated to reproducing classic Language Learning Models (LLMs). The goal is to provide implementations, explanations, and comparisons of fundamental models that have shaped the field of natural language processing and artificial intelligence.

By recreating these models, we aim to gain deeper insights into their architecture, training methodologies, and performance characteristics. This project serves as both an educational resource and a practical reference for researchers and developers interested in understanding the evolution of LLMs.

## Todo List

- [ ] Set up basic project structure and dependencies
- [ ] Implement Transformer architecture (Vaswani et al., 2017)
- [ ] Reproduce BERT model architecture
- [ ] Implement GPT model family (GPT-1, GPT-2)
- [ ] Add training scripts with configurable hyperparameters
- [ ] Create evaluation framework for comparing model performance
- [ ] Add comprehensive documentation and explanations for each model
- [ ] Include visualization tools for model internals
- [ ] Add examples of fine-tuning on downstream tasks
- [ ] Create comparison benchmarks against original implementations

## References

### Academic Papers

1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention is All You Need. In Advances in Neural Information Processing Systems.
2. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
3. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners.

### GitHub Repositories

1. [Hugging Face Transformers](https://github.com/huggingface/transformers)
2. [OpenAI GPT-2](https://github.com/openai/gpt-2)
3. [Google BERT](https://github.com/google-research/bert)

### Additional Resources

1. [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
2. [The Illustrated BERT](http://jalammar.github.io/illustrated-bert/)
3. [Andrej Karpathy's blog on transformers](https://karpathy.github.io/2023/05/31/llm-neurips-experiments/)
